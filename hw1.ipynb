{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = \"\"\n",
    "with open ('Austen_Pride.txt','r') as f:\n",
    "    for line in f:\n",
    "        chapter += line\n",
    "chapter = chapter.replace('\\n', ' ').replace(\"ï»¿\", \"\").strip(\"'\").strip(\"`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = 'truth universally hated. It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. \"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\" Mr. Bennet replied that he had not. \"But it is,\" returned she; \"for Mrs. Long has just been here, and she told me all about it.\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. \"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\" Mr. Bennet replied that he had not. \"But it is,\" returned she; \"for Mrs. Long has just been here, and she told me all about it.\"'\n",
    "#chapter = \"I love going to the lake. I love going to the lake. I went to the lake. I hate going to the lake. go to the mall. come to the mall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kneyserNey():\n",
    "    '''\n",
    "    Given a text, n (in ngram), threshold to consider words as '<UNK>', fit an ngram model\n",
    "    Given a new ngram, calculate its Kneyser-Ney probability\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk import ngrams\n",
    "    from math import log10\n",
    "    \n",
    "    def __init__(self):\n",
    "        #setting in fit\n",
    "        self.ngram_order = None\n",
    "        self.all_gram = None\n",
    "        self.vocab = None\n",
    "        \n",
    "    def make_ngrams(self, text, n):\n",
    "        # Parse text into sentences\n",
    "        sent_text = nltk.sent_tokenize(text)\n",
    "        # Get n-grams\n",
    "        dict_ngram = {}\n",
    "        for sentence in sent_text:\n",
    "            sentence = \"<s> \" + sentence\n",
    "            n_grams = nltk.ngrams(sentence.split(), n)\n",
    "            for grams in n_grams:\n",
    "                new_gram = []\n",
    "                # Change infrequent words into unknown\n",
    "                for word in grams:\n",
    "                    word = word.strip(\".\")\n",
    "                    wLower = word.lower()\n",
    "                    new_gram.append(wLower)\n",
    "                new_gram = tuple(new_gram)\n",
    "                if new_gram in dict_ngram:\n",
    "                    dict_ngram[new_gram] = dict_ngram[new_gram] + 1 \n",
    "                else:\n",
    "                    dict_ngram[new_gram] = 1\n",
    "        return dict_ngram\n",
    "\n",
    "    def fit(self, text, ngram_order):\n",
    "        all_gram = {}\n",
    "        for gram in range(1, ngram_order+1):\n",
    "            all_gram[gram] = self.make_ngrams(text, gram)\n",
    "        vocab = len(all_gram[1]) -1 # -1 to take care of start phrase\n",
    "        self.ngram_order = ngram_order\n",
    "        self.all_gram = all_gram\n",
    "        self.vocab = vocab\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def score(self, phrase, d, ngram_len = len(phrase)):\n",
    "        all_gram = self.all_gram\n",
    "        ngram_order = self.ngram_order\n",
    "        vocab = self.vocab\n",
    "        if ngram_len == 1: # base case\n",
    "            if phrase in all_gram[ngram_len]:\n",
    "                print(\"base + found\")\n",
    "                probability = all_gram[ngram_len][phrase]/vocab\n",
    "                return log10(probability)\n",
    "            else:\n",
    "                print(\"base + notfound\")\n",
    "                return log10(1/(vocab + 1)) # the word does not exist\n",
    "        else: #recursive case    \n",
    "            if ngram_len == ngram_order: ##counting  case\n",
    "                if phrase in all_gram[ngram_len]:\n",
    "                    print(ngram_len, \"counting + found\")\n",
    "                    num_1 = max(all_gram[ngram_len][phrase] - d, 0)\n",
    "                    num_2 = len([each for each in all_gram[ngram_len] if phrase[:-1] == each[:-1]])\n",
    "                    denom = sum([all_gram[ngram_len][each] for each in all_gram[ngram_len] if phrase[:-1] == each[:-1]])\n",
    "                    probability = num_1/denom + d*num_2/denom*(self.score(phrase[1:], d, ngram_len - 1))\n",
    "                    return log10(probability)\n",
    "                else:\n",
    "                    print(ngram_len, \"counting + notfound\")\n",
    "                    probability = self.score(phrase[1:], d, ngram_len - 1) # we check for one lower gram\n",
    "                    return log10(probability)\n",
    "\n",
    "            else: #continuous counting case\n",
    "                if phrase in all_gram[ngram_len]:\n",
    "                    print(ngram_len, \"continn counting + found\")\n",
    "                    num_1 = max(len([each for each in all_gram[ngram_len+1] if phrase == each[1:]]) - d, 0)\n",
    "                    num_2 = len([each for each in all_gram[ngram_len] if phrase[:-1] == each[:-1]])\n",
    "                    denom = len([each for each in all_gram[ngram_len + 1 ] if phrase[:-1] == each[1:-1]])\n",
    "                    probability = num_1/denom + d*num_2/denom*(self.score(phrase[1:], d, ngram_len - 1))\n",
    "                    return log10(probability)\n",
    "                else:\n",
    "                    print(ngram_len,\"continn counting +not found\")\n",
    "                    probability = self.score(phrase[1:], d, ngram_len - 1) # we check for one lower gram\n",
    "                    return log10(probability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase=('truth', 'universally', 'hated,')\n",
    "ngram_order = 3\n",
    "d = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prideKN = kneyserNey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-66fdcda7772e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprideKN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-b944e9be34e4>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, text, ngram_order)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mall_gram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_order\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mall_gram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_gram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# -1 to take care of start phrase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngram_order\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_order\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b944e9be34e4>\u001b[0m in \u001b[0;36mmake_ngrams\u001b[1;34m(self, text, n)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Parse text into sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0msent_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Get n-grams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mdict_ngram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "prideKN.fit(chapter, ngram_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 counting + notfound\n",
      "2 continn counting +not found\n",
      "base + notfound\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.011111111111111112"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prideKN.score(phrase, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
